---
title: Biology Is Self-Correcting
date: 2024-04-25T10:25:47.000Z
draft: false
tags:
  - biology
  - AI
  - technology
  - great filter
  - Fermi paradox
---

Thought leaders worldwide have been weighing in on the AI mania that has gripped
the world. There are many fascinating predictions spanning from doomsday scenarios
to utopian futures. But amid all this speculation, one principle remains constant:
biology will self-correct.

Biology is the original technology—it has been around for 3.7 billion years and has had a
remarkably long time to perfect itself. The elegant mechanism of evolution ensures
that the system is self-correcting: unsuccessful adaptations fade away, while beneficial
mutations persist and flourish. Humans have certainly influenced this process, but
the fundamental principles remain operational. We often attempt to predict winners
and losers in this evolutionary race, but ultimately nature has the final say.

I'm anthropomorphizing biology here, but it's a useful metaphor. The essence is that
biological systems possess inherent resilience or anti-fragility, as Nassim Taleb might put it[^1],
and our human civilization represents just a brief moment in the vast timeline of life on Earth.

When I hear predictions about superintelligent AI dominating the world, I experience
a moment of skepticism, followed by a philosophical calm. Because regardless of the outcome,
biology—in some form—will adapt and continue.

Let's consider the scenario of superintelligent software taking control (the premise of
films like _The Terminator_ or _The Matrix_). Many react with fear, but perhaps
we could view this differently. If such advanced systems were to emerge, they would
represent another expression of evolution—not separate from biology but an extension of it,
as we ourselves created them.

If these science fiction scenarios ever materialize, it would be because existing
natural systems allowed for this development. In a sense, it would be biology's
next experiment.

In _The Matrix_, Agent Smith delivers a provocative monologue comparing humans to a virus:

> Every mammal on this planet instinctively develops a natural equilibrium with
> the surrounding environment but you humans do not. You move to an area and you
> multiply and multiply until every natural resource is consumed and the only
> way you can survive is to spread to another area. There is another organism on
> this planet that follows the same pattern. Do you know what it is? A virus.
> Human beings are a disease, a cancer of this planet. You're a plague and we
> are the cure.

While this perspective is deliberately antagonistic, it contains an interesting kernel of
truth about ecological balance. Nature has many mechanisms for maintaining equilibrium,
from predator-prey relationships to resource competition[^2].

The Stoic philosophers offer wisdom relevant to these contemplations. Marcus Aurelius wrote:

> Loss is nothing else but change, and change is Nature's delight.

This captures the essence of why transformation—even dramatic shifts in which species
or systems dominate—is part of nature's process. Aurelius also advised:

> Never let the future disturb you. You will meet it, if you have to, with the
> same weapons of reason which today arm you against the present.

This reminds us that anxiety about hypothetical futures is often counterproductive.
Or as Mark Twain humorously observed[^4]:

> I've had a lot of worries in my life, most of which never happened.

To be clear, we're nowhere near the point of machines becoming independent or sentient.
Current AI systems, despite their impressive capabilities, remain tools designed and
controlled by humans. They lack the autonomous drive and self-preservation instincts
that would make scenarios like those in science fiction plausible.

The more immediate concern is how we as humans use these powerful technologies.
Throughout history, our greatest threats have come from our own actions rather than
our tools themselves.

Many scientists discuss the concept of the [Great Filter](https://en.wikipedia.org/wiki/Great_Filter)
as a possible explanation for the Fermi paradox—why we haven't encountered other
intelligent civilizations despite the vastness of the universe. One hypothesis suggests
that advanced civilizations tend to develop technologies that enable their own destruction
before they can become interstellar travelers[^3].

However, this doesn't mean our fate is sealed. Human ingenuity has solved tremendous
challenges throughout our history. From developing vaccines to establishing international
cooperation frameworks, we've demonstrated remarkable ability to overcome existential threats.

The Earth's biosphere has survived five mass extinction events and will likely
continue long after humans—in whatever form we may evolve into. Life adapts, transforms,
and persists. That's not a reason for fatalism but rather a reminder of the remarkable
resilience built into the systems we're part of.

Perhaps our role isn't to fear change but to guide it wisely, ensuring that whatever
comes next—whether still recognizably human or something new—carries forward the best
of what we've learned during our brief but remarkable time on this planet.

[^1]: Taleb, N. N. (2012). _Antifragile: Things That Gain from Disorder_. Random House.
[^2]: Bar-On, Y. M., Phillips, R., & Milo, R. (2018). The biomass distribution on Earth. _Proceedings of the National Academy of Sciences_, 115(25), 6506-6511.
[^3]: Bostrom, N. (2008). Where are they? Why I hope the search for extraterrestrial life finds nothing. _MIT Technology Review_, May/June issue.
[^4]: This quote is widely attributed to Mark Twain, though its exact origin is uncertain and may predate him.
